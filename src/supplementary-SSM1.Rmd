---
title: "Supplementary material: The increasing cost of happiness"
bibliography: ../src/references.bib
csl: ../src/elsevier-harvard.csl
output:  
  word_document:
    reference_docx: ../src/Rmarkdown-for-Word-styles.docx
always_allow_html: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
library(brms)
library(flextable)
library(gtsummary)
library(patchwork)

library(bayesplot)

knitr::opts_chunk$set(eval = TRUE,
                      echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.path = "../figures/",
                      ft.align="left")

source("ft_themes.r")
```
\  
\  

Draft: `r format(Sys.time(), '%d %B, %Y')`  
Supplementary tables:      4  
Supplementary figures:     6  
  
**keywords:** Subjective wellbeing, household income, HILDA  

\  

***


## Supplementary Material

<br>


#### Sample characteristics  

The broad demographic characteristics of the sample at each sixth year wave are presented below in Table S1. The linear trend as a function of time was estimated for each variable (linear regression of time for continuous variables and Cochrane-Armitage test in the case of binary variables), and Bon-Ferroni adjusted p-values for multiple comparisons are presented (k = 11).    

##### Table S1. Sample characteristics  
```{r table_s1}
prop_trend_test <- function(data, variable, by, ...) {
  
  data %>%
    select(x = {{variable}}, y = {{by}}) %>%
    dplyr::filter(complete.cases(.)) %>%
    dplyr::group_by(y) %>%
    dplyr::summarise(
      xsum = sum(x, na.rm = T),
      n = n()
    ) %>%
    {prop.trend.test(x = .$xsum, n = .$n)} %>%
    broom::tidy() %>%
    mutate(method = "Cochrane-Armitage test",
           p.value = p.value * 11,
           p.value = if_else(p.value > 1, 1, p.value))
}

linear_trend_test <- function(data, variable, by, ...) {
  
  data %>%
    select(y = {{variable}}, x = {{by}}) %>%
    dplyr::filter(complete.cases(.)) %>%
    {lm(formula = y ~ ordered(x), data = .)} %>%
    broom::tidy() %>%
    filter(str_detect(term, ".L")) %>%
    mutate(method = "linear trend analysis (parametric)",
           p.value = p.value * 11,
           p.value = if_else(p.value > 1, 1, p.value))
  
}

hilda_data <- read_rds("../data/hilda_data.rds")

hilda_data %>%
  filter(year %in% c(2001, 2007, 2013, 2019)) %>%
  filter(!student, !top_hifdip, (!is.na(losat)|!is.na(gh9))) %>%
  mutate(
    `University Graduates` = edu == "Grad",
    ) %>%
  select(
    year, `Household income ($)` = hh_disp_inc_eq,
    `Life satisfaction` = losat, `Happiness` = gh9, 
    Males = male, `Age (years)` = age, `University Graduates`, #Workforce,
    Unemployed = unemployed, Couples = coupled, #`Relationship status`, 
    `Chronic illness` = chronic, SEIFA, 
    `Household size` = hhsize,
  ) %>% 
  tbl_summary(
    by = year,
    missing = "no",
    statistic = 
      list(all_continuous() ~ "{mean} (±{sd})",
           all_categorical() ~ "{n} ({p}%)"),
    digits = 
      list(all_continuous() ~ 1,
           `Household income ($)` ~ 0),
    missing_text = "(Missing)" 
    ) %>%
  add_p(
    test = list(
      all_continuous() ~ "linear_trend_test",
      all_dichotomous() ~ "prop_trend_test"
    )
  ) %>%
  modify_header(stat_by = "{level}\nN = {style_number(n)}") %>%
  as_flex_table() %>%
  align(part = "body") %>%
  valign(valign = "top", part = "header") %>%
  fontsize(size = 9.5, part = "all") %>%
  width(j = 1, width = 1.5) %>%
  width(j = 2:6, width = 1)
```

<br>

Household income and average life satisfaction increased between 2001-2019, while average happiness score decreased slightly over the 19 years. The proportions of each sex were stable over time, as was the proportion of people living as a couple (Couples), average household size and SEIFA index. The percentage unemployed varied with economic conditions however there was no significant positive or negative linear trend. Age and chronic health conditions tended to increase over time. The slight increase in the age of the panel over time (2.3 years) was substantially less than that of a cohort study which would be 18 years. Education levels (i.e., percentage of university graduates) also tended to increase over time.   

Generally, Table S1 supports the view that the HILDA sample was a relatively stable representation of Australian socioeconomic conditions between 2001 and 2019. The importance of the specific demographic variables with a significant linear trend (age, age^2^, education, illness) were tested in regression models of happiness as covariates (see below Figure S3).   

<br><br>

#### Model Comparisons  

We compared the out-of-sample evidence for the piecewise-linear models with the log-linear and linear models of income on wellbeing in each year, using the difference in WAIC deviance scores (i.e., WAIC~piecewise~ — WAIC~log~, WAIC~piecewise~ — WAIC~linear~). Since smaller deviance scores indicate better fit, a difference in deviance scores less than zero (i.e., negative) indicated evidence for the piecewise-linear model; a difference greater than zero (i.e., positive) indicated evidence for the other model (log-linear or linear).   

<br>

##### Figure S1. Model comparisons (WAIC~piecewise~ vs WAIC~linear~, WAIC~log~)
```{r figure_s1, fig.width=6, fig.height = 4, dpi=300}
lm_fits = read_rds("../results/lm_gh9_hhoecd_0cov.RDS")
nl_fits = read_rds("../results/gh9_hhoecd_0cov_tight.RDS")

# get waic results
map2_dfr(.x = nl_fits, 
         .y = lm_fits, 
         .f = ~{
           loo_compare(.x, .y, criterion = "waic") %>%
             as.data.frame() %>%
             rownames_to_column("model") %>%
             as_tibble()
         },
         .id = "year") %>%
  filter(elpd_diff != 0) %>%
  mutate(
    year = extract_numeric(year),
    model = if_else(model == ".x", "nl", "piecewise vs. linear"),
    waic_diff = 2 * elpd_diff, # lm disadvantage
    se = 2 * se_diff,
    wellbeing = "Affective wellbeing"
  ) -> waic_happiness_lm


nl_fits_x0 = read_rds("../results/gh9_hhoecd_0cov_tight_x0.RDS")
log_fits = read_rds("../results/log_gh9_hhoecd_0cov.RDS")

# get waic results
map2_dfr(.x = nl_fits_x0, 
         .y = log_fits, 
         .f = ~{
           loo_compare(.x, .y, criterion = "waic") %>%
             as.data.frame() %>%
             rownames_to_column("model") %>%
             as_tibble()
         },
         .id = "year") %>%
  filter(elpd_diff != 0) %>%
  mutate(
    year = extract_numeric(year),
    model = if_else(model == ".x", "nl", "piecewise vs. log"),
    waic_diff = 2 * elpd_diff, # lm disadvantage
    se = 2 * se_diff,
    wellbeing = "Affective wellbeing"
  ) -> waic_happiness_log

nl_fits_losat = read_rds("../results/losat_hhoecd_0cov.RDS")
lm_fits_losat = read_rds("../results/lm_losat_hhoecd_0cov.RDS")

map2_dfr(.x = nl_fits_losat, 
         .y = lm_fits_losat, 
         .f = ~{
           loo_compare(.x, .y, criterion = "waic") %>%
             as.data.frame() %>%
             rownames_to_column("model") %>%
             as_tibble()
         },
         .id = "year") %>%
  filter(elpd_diff != 0) %>%
  mutate(
    year = extract_numeric(year),
    model = if_else(model == ".x", "nl", "linear"),
    waic_diff = if_else(model == "linear", 
                        2 * elpd_diff,   # nl advantage
                        -2 * elpd_diff), # nl advantage
    se = 2 * se_diff,
    wellbeing = "Cognitive wellbeing"
  ) %>%
  mutate(model = "piecewise vs. linear") -> waic_satisfaction_lm

log_fits_losat = read_rds("../results/fits/excl_topcodes/sd/exc_zeros/log_losat_hhoecd_0cov_x0.RDS")
nl_fits_losat_x0 = read_rds("../results/fits/excl_topcodes/sd/exc_zeros/losat_hhoecd_0cov_tight_x0.RDS")

map2_dfr(.x = nl_fits_losat_x0, 
         .y = log_fits_losat, 
         .f = ~{
           loo_compare(.x, .y, criterion = "waic") %>%
             as.data.frame() %>%
             rownames_to_column("model") %>%
             as_tibble()
         },
         .id = "year") %>%
  filter(elpd_diff != 0) %>%
  mutate(
    year = extract_numeric(year),
    model = if_else(model == ".x", "nl", "piecewise vs. log"),
    waic_diff = if_else(model == "piecewise vs. log", 
                        2 * elpd_diff,   # nl advantage
                        -2 * elpd_diff), # nl advantage
    se = 2 * se_diff,
    wellbeing = "Cognitive wellbeing"
  ) -> waic_satisfaction_log

bind_rows(waic_happiness_lm, 
          waic_happiness_log,
          waic_satisfaction_lm, 
          waic_satisfaction_log) %>%
  ggplot(aes(x = waic_diff, y = year, color = model)) +
    geom_vline(aes(xintercept = 0), color = "grey90") +
    geom_pointrange(aes(xmin = waic_diff - se*1.96, xmax = waic_diff + se*1.96),
                    position = position_dodge(width = .5)) +
    facet_wrap(~wellbeing) +
    scale_y_continuous(breaks = c(2001, 2007, 2013, 2019)) +
    scale_color_manual(values = c("#ffcc80", "#8F2727")) + # "#005b96"
    ft_theme(base_size = 12) +
    theme(legend.title = element_blank(),
          legend.position = "bottom") +
    labs(x = "deviance", y = "")
```

```
Figure legend: Differences in WAIC scores (deviance units) for a piecewise-linear fit over a log-linear fit (red) or over a linear fit (yellow). The filled circle indicates the mean of the difference and the horizontal bars represents the 95% interval. A difference below zero (grey vertical line) represents support for the piecewise model, and a difference above zero is support for the other model.
```  


<br>

The difference in WAIC scores for each model revealed the piecewise-linear fits of affective wellbeing, but not cognitive wellbeing, provided superior out-of-sample accuracy over a log-linear or linear fit in each year. Figure S1 shows the differences in WAIC scores clearly distinguished the advantage of the piecewise-linear fits for affective wellbeing with most 95% intervals entirely below zero for each year (exceptions occurred for the piecewise vs. log fit in 2004, 2006 and 2008, where the piecewise fit was still superior on average). By contrast, the difference in WAIC scores for the cognitive wellbeing models was not as clearly distinguished from zero for most years, since the 95% intervals of the WAIC differences with either the log-linear fit or the linear fit overlapped zero in most years. Indeed in the years between 2002 to 2010 the linear fit was superior to the piecewise-linear fit on average, with the mean WAIC difference above zero (positive). Overall the evidence suggested that affective wellbeing has a piecewise relationship with household income; implying the existence of a change point or difference in slopes at different levels of household income. Evidence of the effect of income on cognitive wellbeing, on the other hand, was more equivocal, albeit with some indication of a linear relationship in the early part of the 21st C. Others have also noted a linear relationship between cognitive wellbeing (life satisfaction) and income [@howell2008relation; @stevenson2013subjective], and the results of our formal comparison provides some support for this notion.    

**Conditional vs unconditional models of happiness.** Table S1 (above) showed the specific variables that changed over the 19 years between 2001-2019. In particular, the number of university graduates, average age and the incidence of chronic illness increased slighty over the period. Including these variables as covariates in any model (i.e,. a conditional model such that *P*( $y$ | income, age, education illness)) will improve the in-sample fit (e.g., R^2^), and also adjust the parameter estimates of interest (e.g., change point) conditional upon their common association with happiness. Given the general aim of the main paper was to describe the temporal change in functional form between happiness and income in Australia, regardless of any other variable (i.e., an unconditional model of *P*($y$| income)) or sampling differences, neither the in-sample fit nor the conditional model estimates are directly relevant. Nevertheless, the importance of these variables in mediating the changes we observed in the unconditional model are relevant, and so we calculated the estimates from conditional piecewise models including age, age^2^, sex, education  (university degree), chronic illness for each year below to confirm the pattern of temporal change was similar. Figure S2 shows the addition of these covariates did not change the trend in change point parameter estimates (red) relative to the unconditional parameter estimates (grey) over the 19 years.  

<br>

##### Figure S2. Conditional model parameters  
```{r figure_s2, fig.width = 6, fig.height = 8, dpi=300}
nl_fits = read_rds("../results/gh9_hhoecd_0cov_tight.RDS")

nl_fits_5cov = read_rds("../results/gh9_hhoecd_5cov_tight.RDS")

income_params <- read_rds("../results/gh9_hhoecd_summary.RDS") %>%
  bind_rows(.id = "fit")

posteriors_5c <- map_dfr(nl_fits_5cov, as.data.frame, .id = "fit") %>%
  left_join(income_params) %>%
  mutate(omega_dollars = (b_omega_Intercept * sd + mean)/1000) %>%
  as_tibble()

param_key = c(
  omega_dollars = "Change point ($000s)",
  b_b0_Intercept = paste0("Intercept (\u03b2", "0)"),
  b_b1_Intercept = paste0("Pre-slope (\u03b2", "1)"),
  b_b2_Intercept = paste0("Post-slope (\u03b2", "2)")
)

posteriors_5c %>%
  select(fit, contains("b0"), contains("b1"), contains("b2"), omega_dollars) %>%
  gather(key = "param", val = "sample", -fit) %>%
  mutate(
    year = extract_numeric(fit),
    param = recode_factor(param, !!!param_key)
  ) %>% 
  group_by(param, year) %>%
  summarise(
    low = quantile(sample, probs = 0.05, na.rm=T),
    expected = quantile(sample, probs = 0.5, na.rm=T),
    upp = quantile(sample, probs = 0.95, na.rm=T)
  ) %>%
  mutate(model = "conditional") -> parameter_summary_5c

posteriors_0c <- map_dfr(nl_fits, as.data.frame, .id = "fit") %>%
  left_join(income_params) %>%
  mutate(omega_dollars = (b_omega_Intercept * sd + mean)/1000) %>%
  as_tibble()

posteriors_0c %>%
  select(fit, contains("b0"), contains("b1"), contains("b2"), omega_dollars) %>%
  gather(key = "param", val = "sample", -fit) %>%
  mutate(
    year = extract_numeric(fit),
    param = recode_factor(param, !!!param_key)
  ) %>% 
  group_by(param, year) %>%
  summarise(
    low = quantile(sample, probs = 0.05, na.rm=T),
    expected = quantile(sample, probs = 0.5, na.rm=T),
    upp = quantile(sample, probs = 0.95, na.rm=T)
  ) %>%
  mutate(model = "unconditional") -> parameter_summary_0c

bind_rows(parameter_summary_0c, parameter_summary_5c) %>%
  ggplot(aes(y = year, x = expected, color = model)) +
  geom_pointrange(aes(xmin = low, xmax = upp), position = position_dodge(width = 0.75)) +
  scale_y_continuous(breaks = c(2001, 2007, 2013, 2019)) +
  facet_wrap(~param, scales = "free_x") +
  ft_theme(base_size = 12) +
  scale_color_manual(values = c("#C79999", "grey65")) +
  theme(legend.position = "none",
        axis.title = element_blank())
```
```
Figure legend: Comparison of the conditional parameter estimates (red) and unconditional parameter estimates (grey) in the change point model of income on happiness. The conditional models included covariates for age, age squared, sex, education, and illness. Horizontal bar represents the 95% credible region and the solid point indicates the expected value (median) of each distribution.
```  

<br>

The parameter estimates of the conditional model (red) were adjusted for the broad demographic variables which tended to increase over the 19 years in our sample (age, education, illness); however the pattern of changes over time was comparable to the unconditional model (grey), indicating these temporal trends were not responsible for the evolution in the change point between income and happiness.  

The unconditional parameter values are also presented in Table S2 below (Parameter and fit summary statistics).  

<br><br>

#### Parameter and fit summary statistics  

Model fitting was performed in RStan (v2.21.2) using the brms (v2.14.4) package [@burkner2017; @rstan2019]. There were _N_ = 59,876 total observations. For each Bayesian model presented in this report, 4000 samples (post-warmup) were drawn using sampling(NUTS).   

For all models estimated with RStan presented in this report we confirmed the absence of divergent transitions; the $\hat{R}$ approached 1 indicating convergance; and the effective sample size (ESS) for each parameter was _n_ > 300, which is a conservative threshold for estimation.  

Table S2 below shows the fit and summary statistics for each piecewise-linear model fit of affective wellbeing and income in each year. Estimates for the intercept, pre-slope and post-slope are in year-wise SD units while the change-point is presented in real 2019 dollars.    

##### Table S2. Parameter estimates and fit statistics for piecewise-linear fit of affective wellbeing and income
```{r table_s2, ft.align="left"}
nl_fits = read_rds("../results/gh9_hhoecd_0cov_tight.RDS")

income_params %>%
  mutate(fit = str_remove(fit, "fit")) %>%
  column_to_rownames("fit") -> income

param_key = c(
  omega_Intercept = "Change point ($000s)",
  b0_Intercept = paste0("Intercept (\u03b2", "0)"),
  b1_Intercept = paste0("Pre-slope (\u03b2", "1)"),
  b2_Intercept = paste0("Post-slope (\u03b2", "2)")
)

# param_key = c(
#   b0_Intercept = "Intercept",
#   b1_Intercept = "Pre-slope",
#   b2_Intercept = "Post-slope",
#   omega_Intercept = "Change-point ($000s)"
# )

map_dfr(nl_fits, 
        .f = ~{
          fit_summary <- summary(.x, prob = 0.95) 
          
          sigma_summary <- fit_summary$spec_pars %>%
            as_tibble(rownames = "Parameter")
          
          fit_summary$fixed %>%
            as_tibble(rownames = "Parameter") %>%
            bind_rows(sigma_summary) %>%
            select(Parameter, everything())
          
          },
        .id = "Year") %>%
  mutate(
    Year = str_remove(Year, "fit"),
    Parameter = recode_factor(Parameter, !!!param_key),
    Estimate = if_else(str_detect(Parameter, "Change"),
                       (Estimate * income[Year, "sd"] + income[Year, "mean"])/1000,
                       Estimate),
    Est.Error = if_else(str_detect(Parameter, "Change"),
                        (Est.Error * income[Year, "sd"] + income[Year, "mean"])/1000,
                        Est.Error),
    `l-95% CI` = if_else(str_detect(Parameter, "Change"),
                        (`l-95% CI` * income[Year, "sd"] + income[Year, "mean"])/1000,
                        `l-95% CI`),
    `u-95% CI` = if_else(str_detect(Parameter, "Change"),
                         (`u-95% CI` * income[Year, "sd"] + income[Year, "mean"])/1000,
                         `u-95% CI`)
    ) %>%
  flextable() %>%
  merge_v(j = 1) %>%
  valign(j = 1, valign = "top") %>%
  bg(i = ~Year %in% seq(2001, 2019, 2), 
     bg = "grey90", 
     part = "body") %>%
  colformat_double(j = 3:6, digits= 3) %>%
  colformat_double(j = 7, digits= 1) %>%
  footnote(i = 1, j = 7,
           value = as_paragraph(
             c("Rhat is the potential scale reduction factor on split chains")),
           ref_symbols = c("a"),
           part = "header") %>%
  footnote(i = 1, j = c(8:9),
           value = as_paragraph(
             c("Bulk_ESS and Tail_ESS are effective sample size (ESS) measures")),
           ref_symbols = c("b"),
           part = "header") %>%
  fontsize(size = 9, part = "all") %>%
  width(j = 1, width = .5) %>%
  width(j = 2, width = 1) %>%
  width(j = c(3:7), width = .7) %>%
  width(j = c(8, 9), width = .8)
```

<br><br>

The coeficients (in SD units) for the piecewise-linear, log-linear and linear Bayesian models are shown in the table below.  

##### Table S3. Bayesian coefficients from linear, log-linear and piecewise-linear models  
```{r table_s3}
map_df(log_fits, .f = ~{
  fixef(.x) %>%
    as_tibble(rownames = "terms") %>%
    transmute(terms, coef = paste0(round(Estimate, 2), " (±", round(Est.Error, 3), ")")) %>%
    spread(terms, coef)},
  .id = "Year"
  ) %>%
  rename("b0_log" = 2, "b1_log" = 3) -> coefs_gh9_log

map_df(lm_fits, .f = ~{
  fixef(.x) %>%
    as_tibble(rownames = "terms") %>%
    transmute(terms, coef = paste0(round(Estimate, 2), " (±", round(Est.Error, 3), ")")) %>%
    spread(terms, coef)},
  .id = "Year"
  ) %>%
  select(Year, "b0_lin" = 3, "b1_lin" = 2) -> coefs_gh9_lm

map_df(nl_fits, .f = ~{
  fixef(.x) %>%
    as_tibble(rownames = "terms") %>%
    transmute(terms, coef = paste0(round(Estimate, 2), " (±", round(Est.Error, 3), ")")) %>%
    spread(terms, coef)},
  .id = "Year"
  ) %>%
  rename("b0_pw" = 2, "b1_pw" = 3, 
         "b2_pw" = 4, "Omega" = 5) -> coefs_gh9_pw

coefs_affective <- left_join(coefs_gh9_pw, coefs_gh9_log) %>%
  left_join(coefs_gh9_lm) %>%
  mutate(Year = str_remove(Year, "fit"),
         Wellbeing = "Affective")

map_df(log_fits_losat, .f = ~{
  fixef(.x) %>%
    as_tibble(rownames = "terms") %>%
    transmute(terms, coef = paste0(round(Estimate, 2), " (±", round(Est.Error, 3), ")")) %>%
    spread(terms, coef)},
  .id = "Year"
) %>%
  rename("b0_log" = 2, "b1_log" = 3) -> coefs_losat_log

map_df(lm_fits_losat, .f = ~{
  fixef(.x) %>%
    as_tibble(rownames = "terms") %>%
    transmute(terms, coef = paste0(round(Estimate, 2), " (±", round(Est.Error, 3), ")")) %>%
    spread(terms, coef)},
  .id = "Year"
) %>%
  select(Year, "b0_lin" = 3, "b1_lin" = 2) -> coefs_losat_lm

map_df(nl_fits_losat, .f = ~{
  fixef(.x) %>%
    as_tibble(rownames = "terms") %>%
    transmute(terms, coef = paste0(round(Estimate, 2), " (±", round(Est.Error, 3), ")")) %>%
    spread(terms, coef)},
  .id = "Year"
) %>%
  rename("b0_pw" = 2, "b1_pw" = 3, 
         "b2_pw" = 4, "Omega" = 5) -> coefs_losat_pw

coefs_cognitive <- left_join(coefs_losat_pw, coefs_losat_log) %>%
  left_join(coefs_losat_lm) %>%
  mutate(Year = str_remove(Year, "fit"),
         Wellbeing = "Cognitive")


bind_rows(coefs_affective, coefs_cognitive) %>%
  select(Wellbeing, everything()) %>%
  flextable() %>%
  merge_v(j = 1) %>%
  set_header_labels(values = c(b0_pw = paste0("\u03b2", "0"),
                               b1_pw = paste0("\u03b2", "1"),
                               b2_pw = paste0("\u03b2", "2"),
                               Omega = "\u03C9",
                               b0_log = paste0("\u03b2", "0"),
                               b1_log = paste0("\u03b2", "1"),
                               b0_lin = paste0("\u03b2", "0"),
                               b1_lin = paste0("\u03b2", "1"))) %>%
  add_header(top = TRUE,
             values = c(b0_pw = "Piecewise-linear (±se)",
                        b1_pw = "Piecewise-linear (±se)",
                        b2_pw = "Piecewise-linear (±se)",
                        Omega = "Piecewise-linear (±se)",
                        b0_log = "Log-linear (±se)",
                        b1_log = "Log-linear (±se)",
                        b0_lin = "Linear (±se)",
                        b1_lin = "Linear (±se)")) %>%
  merge_h(part = "header") %>%
  width(j = 1:2, width = c(0.75, 0.5)) %>%
  width(j = 3:10, width = 0.65) %>%
  fontsize(size = 9, part = "all") %>%
  # flextable::theme_vanilla() %>% # theme_vanilla() 
  valign(valign = "top") %>%
  valign(valign = "bottom", part = "header") %>%
  border_remove() %>%
  hline_top(part="header", border = officer::fp_border()) %>%
  hline_bottom(part="header", border = officer::fp_border()) %>%
  hline(i = 19, border = officer::fp_border()) %>%
  hline_bottom(part="body", border = officer::fp_border())
```

<br><br>

#### Prior and residual checks   

We used regularizing Guassian priors centred on zero, with Normal(0, 0.1) for each *&beta;*, and a Normal(0, 0.5) for *&omega;*. A plot of the joint distribution between income and wellbeing imposed by the priors alone confirmed there was no apparent slope or change point implied by their regularizing effect.  

##### Figure S3. Prior predictive check  
```{r figure_s3, fig.width=6, fig.height = 6, dpi=300}
load("../results/prior_predictions.Rdata")

prior_predictions %>%
  ggplot(aes(x = hh_disp_inc_eq/1000, y = Estimate, group = 1)) +
  geom_point(size = 0.1, alpha = 0.1) +
  stat_smooth(color = "#8F2727", size = 0.5, se = F) +
  xlim(c(0, 300)) +
  labs(y = "Wellbeing estimate (SD units)", x = "Household income ($000s)") +
  theme_test()
```
```
Figure legend: The joint distribution between wellbeing and income for a single year, imposed by the priors. The solid blue line indicates a smoothed average (loess).
```  
<br><br>

Examination of the residual plots from the piecewise models of happiness confirms the residuals were evenly distributed around zero and there was little indication of heteroscedascity. Overall, there was little evidence that the piecewise models of happiness were not appropriate for the data.  

<br>

##### Figure S4. Residual plots for piecewise happiness models
```{r figure_s4, fig.height=9, fig.width=9, dpi=600}
# https://www.flutterbys.com.au/stats/tut/tut7.2b.html
# 
# 
# residual_fits <- map_dfr(nl_fits, ~{
#   
#   d0 <- resid(.x) %>%
#     as_tibble() %>%
#     select(resids = Estimate)
#   
#   d1 <- fitted(.x) %>%
#     as_tibble() %>%
#     select(fitteds = Estimate)
#   
#   bind_cols(d0, d1)
#   
#   }, .id = "fit") %>%
#   mutate(year = extract_numeric(fit))
# 
# save(residual_fits, file = paste0(here, "results/residual_fits.Rdata"))

load("../results/residual_fits.Rdata")

residual_fits %>%
  group_by(year) %>%
  slice_sample(n = 1000) %>% # to reduce overplotting
  ggplot(aes(x = fitteds, y = resids)) +
    geom_point(alpha = 0.1, size = 0.1) +
    facet_wrap(~year, scales = "free") +
    theme_test()
```


```
Figure legend: The joint distribution between residual and fitted values for each year.
``` 

<br><br>

#### Model predictions (2001-2019)

The posterior estimates of the piecewise model indicated that the relationship between happiness and income changed over time between 2001 and 2019. One implication of such a change is the disparity in happiness between income groups has increased. This will occur as more people fall below the change point over time and so are subject to the steep region of the function where income and happiness are strongly related. To directly examine the implications of our model for such inequities in happiness, we used the model to estimate or _predict_ the happiness of two different income levels: one income level which was above the change point in 2001 but fell below it by 2019 (\$50K/yr); and another level which always remained above the change points over the same period (\$75K/yr). Figure S8 below presents the happiness levels (in SD units/year) for each income level as well as the difference in happiness between them (∆).  

<br>

##### Figure S5. Happiness (SD units) at $50K/yr and $75K/yr from 2001-2019  
```{r figure_s5, fig.width=7, fig.height=5, dpi=300}
nl_fits = read_rds("../results/gh9_hhoecd_0cov_tight.RDS")

# Optional
# nl_fits_5cov = read_rds(
#   paste0(here, "results/fits/excl_topcodes/sd/gh9_hhoecd_5cov_tight.RDS")
# )

fixed_income_hash <- read_rds("../results/gh9_hhoecd_summary.RDS") %>%
  bind_rows(.id = "fit") %>%
  mutate(low = (40000 - mean)/sd,
         high = (75000 - mean)/sd) %>%
  column_to_rownames(var = "fit") %>%
  as.matrix()

colnames(fixed_income_hash) <- NULL

fixed_income_fit <- tibble()

for (name in names(nl_fits)) {
  
  newdat = data.frame(
    dollars = fixed_income_hash[name, 3:4]
  )
  
  df <- fitted(nl_fits[[name]], newdata = newdat) %>% 
    as_tibble() %>%
    mutate(Dollars = c("$50K", "$75K"),
           
           fit = name)
  
  fixed_income_fit <- bind_rows(fixed_income_fit, df)
  
}

fixed_income_fit %>%
  mutate(year = extract_numeric(fit)) %>%
  select(Estimate, Dollars, year) %>%
  filter(Dollars %in% c("$75K", "$50K")) %>%
  spread(Dollars, Estimate) %>%
  mutate(`$75K - $50K (∆)` = `$75K` - `$50K`) %>%
  gather(income, happiness, -year) %>%
  mutate(income = fct_relevel(income, "$50K", "$75K")) %>%
  ggplot(aes(x = year, y = happiness, color = income)) +
  geom_point(size = 4, alpha = 0.25) +
  geom_smooth(method = "loess", se = F, span = 3) +
  facet_wrap(~income) +
  labs(caption = "Source:     HILDA, University of Melbourne
                 Predicted happiness scores (SD unit) from piecewise regression on real household income", 
       x = "", y = "") -> p


p +
  ft_theme() +
  ggthemes::scale_colour_wsj() +
  theme(legend.position = "none",
        plot.title = element_text(size = 19),
        strip.text = element_text(size = 12))
```

```
Figure S6 legend: The difference (∆) in happiness between a household income of $50K per year and a household income of $75K per year has increased between 2001 and 2019.
```  

<br>

Figure S6 shows the disparity in happiness between two fixed income levels, \$50K/year (traversing the change points) and \$75K/year (above the change points). In 2001, a household income of \$50K/year achieved an above average level of happiness relative to everyone else that year, however by 2019 happiness had declined to lower than average levels relative to the population. By contrast, a household income of \$75K/year enjoyed a higher than average level of happiness for the entire period. The difference (∆) in happiness between these two income levels doubled over the period. The increasing disparity in happiness between the rich and the poor implies that happiness has became more inequitable over time.  


<br><br>


## Appendix  

#### OLS Insample fit statistics

Our primary objective was to understand the relationships between the parameters of our model, and so in-sample fit statistics such as R^2^ are irrelevant for this objective. Moreover, our use of regularized priors will bias such in-sample estimates towards zero as regularization deliberately sacrifices in-sample variance for out-of-sample accuracy, which obviously hinders interpretation of the in-sample values. Nevertheless, some readers may find it useful to compare the in-sample fit statistics between the three different model types employed here: piecewise, linear and log-linear, as well as between the conditional and unconditional models. Because the usual fit statistics such as R^2^ present a problem for Bayesian fits, as the variance of the predicted values can be larger than the variance of the data results in R^2^ values greater than 1, we present the in-sample statistics from OLS model fits (without regularization or penalized likelihoods).   

```{r model_summaries_yearly}
library(segmented)

hilda_data %>%
  filter(
    !student,
    !top_hifdip
  ) %>%
  filter(hh_disp_inc_eq > 0) %>%
  group_by(year) %>%
  transmute(
    y = as.vector(scale(gh9)),
    dollars = hh_disp_inc_eq/10000,
    male = male,
    age = as.vector(scale(age)),
    age_sq = age^2,
    grad = edu %in% c("Grad", "Postgrad"),
    illness = chronic,
    weights = as.vector(scale(weights))
  ) %>%
  na.omit() %>%
  nest() -> nested_data

nested_data %>%
  mutate(
    pw_fit = map(data, ~segmented(lm(
      formula = y ~ dollars, 
      data = .x),
      seg.Z = ~dollars)),
    pw_fit_c = map(data, ~segmented(lm(
      formula = y ~ dollars + male + age + age_sq + grad + illness + weights, 
      data = .x),
      seg.Z = ~dollars)),
    lin_fit = map(data, ~lm(
      formula = y ~ dollars, 
      data = .x)),
    lin_fit_c = map(data, ~lm(
      formula = y ~ dollars + male + age + age_sq + grad + illness + weights, 
      data = .x)),
    log_fit = map(data, ~lm(
      formula = y ~ log(dollars), 
      data = .x)),
    log_fit_c = map(data, ~lm(
      formula = y ~ log(dollars) + male + age + age_sq + grad + illness + weights, 
      data = .x))
    ) -> nested_models

bind_rows(
  map_dfr(nested_models$log_fit, broom::glance) %>%
    mutate(model = "log", year = 2001:2019),
  map_dfr(nested_models$lin_fit, broom::glance) %>%
    mutate(model = "lin", year = 2001:2019),
  map_dfr(nested_models$pw_fit, broom::glance) %>%
    mutate(model = "pw", year = 2001:2019),
  map_dfr(nested_models$log_fit_c, broom::glance) %>%
    mutate(model = "logc", year = 2001:2019),
  map_dfr(nested_models$lin_fit_c, broom::glance) %>%
    mutate(model = "linc", year = 2001:2019),
  map_dfr(nested_models$pw_fit_c, broom::glance) %>%
    mutate(model = "pwc", year = 2001:2019)
) -> model_summaries
```
```{r model_summaries_allyears}
df <- unnest(nested_data, cols = c(data)) %>% ungroup()

#### Testing log-linear vs piecewise all years ####
lm_fit = lm(formula = y ~ dollars, 
             data = df)

lm_fit_c = lm(
  formula = y ~ dollars + male + age + age_sq + grad + illness + weights,
  data = df)


log_fit = lm(formula = y ~ log(dollars), 
             data = df)

log_fit_c = lm(
  formula = y ~ log(dollars) + male + age + age_sq + grad + illness + weights,
  data = df)


pw_fit = segmented(lm(formula = y ~ dollars,
                      data = df),
                   seg.Z = ~dollars)

pw_fit_c = segmented(lm(
  formula = y ~ dollars + male + age + age_sq + grad + illness + weights, 
  data = df),
  seg.Z = ~dollars)

model_summaries_all <- rbind(
  broom::glance(lm_fit) %>% mutate(model = "lin", year = "All"),
  broom::glance(lm_fit_c) %>% mutate(model = "linc", year = "All"),
  broom::glance(log_fit) %>% mutate(model = "log", year = "All"),
  broom::glance(log_fit_c) %>% mutate(model = "logc", year = "All"),
  broom::glance(pw_fit) %>% mutate(model = "pw", year = "All"),
  broom::glance(pw_fit_c) %>% mutate(model = "pwc", year = "All"),
  model_summaries
)
```

##### Table S4. In-sample fit statistics (adj. R-squared)  
```{r table_s4}
model_summaries_all %>%
  select(model, Year = year, adj.r.squared) %>%
  spread(model, adj.r.squared) %>%
  flextable() %>%
  colformat_double(j = 2:7, digits = 3) %>%
  set_header_labels(
    lin = "Linear", 
    linc = "Linear\n(conditional)",
    log = "Log-linear",
    logc = "Log-linear\n(conditional)",
    pw = "Piecewise",
    pwc = "Piecewise\n(conditional)") %>%
  valign(i = 1, valign = "top", part = "header") %>%
  fontsize(size = 9.5, part = "all") %>%
  width(j = 2:7, width = 1) %>%
  width(j = 1, width = .5)
```

<br>

A comparison of the relative differences in R^2^ values between models indicates the piecewise model produced a slightly higher in-sample fit over the other two models, while the linear model tended to have the lowest R^2^. The log-linear fit varied between both over the 19 years. As expected, adding covariates increased the R-squared in each case, but it did not change the relative difference between models.  

Three things need to be observed when interpreting the R^2^ values: 1) While the R^2^ between income and happiness is low (~2% and ~14% in the unconditional and conditional models, respectively), the effect over the wider population will aggregate to be much larger; 2) Happiness (and to a lesser extent, income) is an imperfect and noisy measure so any effect of income on happiness is likely to be larger than that measured here; 3) Finally, happiness is measured on a subjective and arbitrary Likert scale so a small change in happiness at different points in the scale may represent a large change in some other functional/real-world outcome (e.g., risk of suicide). This is all to say that quantifying the real-world impact of small changes in variation reported here is difficult and speculative without further investigation.  

<br><br>

```{r table_s5}
lm_coef <- map_dfr(nested_models$lin_fit, broom::tidy, .id = "Year") %>%
  filter(term == "dollars") %>%
  mutate(coefficient = paste0(round(estimate, 3), " (±", round(std.error, 3), ")")) %>%
  select(Year, `linear (±se)` = coefficient)
  

log_coef <- map_dfr(nested_models$log_fit, broom::tidy, .id = "Year") %>%
  filter(term == "log(dollars)") %>%
  mutate(coefficient = paste0(round(estimate, 3), " (±", round(std.error, 3), ")")) %>%
  select(Year, `log-linear (±se)` = coefficient)

pw_coefs <- map_dfr(nested_models$pw_fit, broom::tidy, .id = "Year") %>%
  filter(term %in% c("dollars", "U1.dollars")) %>%
  select(Year, term, estimate, std.error) %>%
  pivot_wider(id_cols = Year, 
              names_from = term, 
              values_from = estimate:std.error) %>%
  mutate(
    estimate_post = estimate_U1.dollars + estimate_dollars,
    `pre-slope (±se)` = paste0(
      round(estimate_dollars, 3), " ±", round(std.error_dollars, 3)),
    `post-slope (±se)` = paste0(
      round(estimate_post, 3), " ±", round(std.error_U1.dollars, 3))
    ) %>%
  select(Year, `pre-slope (±se)`, `post-slope (±se)`)

pw_coefs <- map_dfr(nested_models$pw_fit, ~as_tibble(.x$psi), .id = "Year") %>%
  mutate(term = "psi.dollars") %>%
  mutate(coefficient = paste0(round(`Est.`, 2)*10, " (±", round(`St.Err`, 2)*10, ")")) %>%
  select(Year, `change point ($000s ±se)` = coefficient) %>%
  left_join(pw_coefs)

left_join(lm_coef, log_coef, by = "Year") %>%
  left_join(pw_coefs, by = "Year") %>%
  mutate(Year = as.character(2000 + as.numeric(Year))) %>%
  flextable() %>%
  set_caption("OLS coefficients from linear, log-linear and piecewise models") %>%
  autofit()
```

<br><br>

## References

